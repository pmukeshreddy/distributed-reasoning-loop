# Distributed Reasoning Loop

End-to-end pipeline for training reasoning models using synthetic data generation, distributed verification, and reinforcement learning.

## ğŸ¯ Results

| Metric | Base Model | GRPO Trained | Improvement |
|--------|------------|--------------|-------------|
| Pass@1 | 35.0% | 55.0% | **+20.0%** |
| Pass@4 | 65.0% | 75.0% | **+10.0%** |
| Pass@8 | 80.0% | 90.0% | **+10.0%** |

## âš¡ Performance

| Component | Metric | Value |
|-----------|--------|-------|
| **SGLang** | Batched speedup | **2.5x** vs sequential |
| **Ray** | Parallelization efficiency | **99.2%** |
| **GRPO** | Trainable params | **0.07%** (LoRA) |
| **Pipeline** | End-to-end | **12 min** on 1x H100 |
| **Result** | Pass@1 accuracy | **+20%** improvement |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED REASONING LOOP                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   SGLang     â”‚ -> â”‚     Ray      â”‚ -> â”‚    GRPO      â”‚       â”‚
â”‚  â”‚  Generation  â”‚    â”‚ Verification â”‚    â”‚   Training   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                  â”‚
â”‚  â€¢ RadixAttention    â€¢ 4 parallel       â€¢ No reward model       â”‚
â”‚  â€¢ Prefix caching      workers          â€¢ Group-relative        â”‚
â”‚  â€¢ 10 paths/problem  â€¢ Math: SymPy        advantages            â”‚
â”‚  â€¢ Batched requests  â€¢ Code: Docker     â€¢ LoRA fine-tuning      â”‚
â”‚                        sandbox                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install sglang[all] ray[default] transformers peft trl datasets omegaconf accelerate bitsandbytes
apt-get install -y libnuma1
```

### Run Full Pipeline

```bash
# 1. Start SGLang server
nohup python -m sglang.launch_server \
    --model-path Qwen/Qwen2.5-1.5B-Instruct \
    --port 30000 --host 0.0.0.0 > sglang.log 2>&1 &
sleep 90

# 2. Run pipeline: SGLang â†’ Ray â†’ GRPO
python scripts/run_ray_pipeline.py
```

### Evaluate
```bash
python scripts/eval_pass_at_k.py \
    --model ./outputs/grpo_model \
    --dataset gsm8k \
    --k 1 4 8 \
    --num-problems 100
```

## ğŸ“ Project Structure

```
distributed-reasoning-loop/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generator/
â”‚   â”‚   â”œâ”€â”€ cot_generator.py          # SGLang/vLLM inference
â”‚   â”‚   â”œâ”€â”€ synthetic_data_pipeline.py
â”‚   â”‚   â””â”€â”€ dataset_loader.py         # GSM8K, HumanEval, MATH, MBPP
â”‚   â”œâ”€â”€ verifier/
â”‚   â”‚   â”œâ”€â”€ math_verifier.py          # SymPy symbolic verification
â”‚   â”‚   â””â”€â”€ code_verifier.py          # Docker sandbox execution
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”‚   â”œâ”€â”€ ray_workers.py            # Distributed processing
â”‚   â”‚   â””â”€â”€ kafka_streaming.py        # Streaming pipeline
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ grpo_trainer.py           # Group Relative Policy Optimization
â”‚   â”‚   â”œâ”€â”€ dpo_trainer.py            # Direct Preference Optimization
â”‚   â”‚   â””â”€â”€ sft_trainer.py            # Supervised Fine-Tuning
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ benchmarks.py             # GSM8K, HumanEval evaluators
â”‚       â””â”€â”€ test_time_compute.py      # Best-of-N, MCTS, Self-Consistency
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_ray_pipeline.py           # Full SGLang â†’ Ray â†’ GRPO pipeline
â”‚   â””â”€â”€ eval_pass_at_k.py             # Pass@k evaluation
â””â”€â”€ config/
    â””â”€â”€ default.yaml                  # Pipeline configuration
```

## ğŸ”§ Key Components

### 1. SGLang Generation
- **RadixAttention**: Automatic prefix caching for shared prompts
- **Batched inference**: 32 concurrent requests via ThreadPoolExecutor
- **Multi-path sampling**: 10 reasoning paths per problem

### 2. Ray Distributed Verification
- **Parallel workers**: 4 actors, 1250 samples each
- **Math verification**: SymPy symbolic comparison
- **Code verification**: Docker sandbox (256MB RAM, 30s timeout)

### 3. GRPO Training (DeepSeek-R1 Approach)
- **No reward model**: Uses group-relative advantages
- **Verification-based**: Correct = positive, incorrect = negative
- **Efficient**: LoRA (r=16, alpha=32), 8-bit quantization

## ğŸ“Š Pipeline Stats

```
Dataset:           GSM8K (500 problems)
Paths generated:   5,000 (10 per problem)
Correct paths:     1,154 (23%)
Incorrect paths:   3,846 (77%)
DPO pairs:         2,090
Training epochs:   5
Final loss:        -0.1119
```

## ğŸ“š References

- [DeepSeek-R1](https://arxiv.org/abs/2401.02954) - GRPO algorithm
- [SGLang](https://github.com/sgl-project/sglang) - RadixAttention inference
- [Ray](https://ray.io/) - Distributed computing
- [GSM8K](https://arxiv.org/abs/2110.14168) - Math reasoning benchmark
