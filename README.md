# Distributed Reasoning Loop

End-to-end pipeline for training reasoning models using synthetic data generation, distributed verification, and reinforcement learning.

## ğŸ¯ Results

| Metric | Base Model | GRPO Trained | Improvement |
|--------|------------|--------------|-------------|
| Pass@1 | 35.0% | 55.0% | **+20.0%** |
| Pass@4 | 65.0% | 75.0% | **+10.0%** |
| Pass@8 | 70.0% | 85.0% | **+15.0%** |

## âš¡ Performance

| Component | Metric | Value |
|-----------|--------|-------|
| **SGLang** | Generation | **3.5 min** for 5K samples |
| **Ray** | Workers | **4 parallel**, balanced distribution |
| **GRPO** | Trainable params | **0.07%** (LoRA) |
| **Pipeline** | End-to-end | **~12 min** on 1x H100 |

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED REASONING LOOP                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   SGLang     â”‚ -> â”‚     Ray      â”‚ -> â”‚    GRPO      â”‚       â”‚
â”‚  â”‚  Generation  â”‚    â”‚ Verification â”‚    â”‚   Training   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                  â”‚
â”‚  â€¢ RadixAttention    â€¢ 4 parallel       â€¢ No reward model       â”‚
â”‚  â€¢ Prefix caching      workers          â€¢ Group-relative        â”‚
â”‚  â€¢ 10 paths/problem  â€¢ Math: SymPy        advantages            â”‚
â”‚  â€¢ Batched requests  â€¢ Code: Docker     â€¢ LoRA fine-tuning      â”‚
â”‚                        sandbox                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install sglang[all] ray[default] transformers peft trl datasets omegaconf accelerate bitsandbytes jinja2 --upgrade
apt-get install -y python-is-python3 python3-pip
```

### Run Full Pipeline
```bash
# 1. Apply fixes
sed -i 's/from verifier import/from src.verifier import/g' src/orchestration/ray_workers.py
sed -i 's/chunk_size = (len(data) + num_workers - 1) \/\/ num_workers/chunk_size = max(1, (len(data) + num_workers - 1) \/\/ num_workers)/' src/orchestration/ray_workers.py

# 2. Start SGLang server (use GPU 1 if multi-GPU)
CUDA_VISIBLE_DEVICES=1 nohup python -m sglang.launch_server \
    --model-path Qwen/Qwen2.5-1.5B-Instruct \
    --port 30000 --host 0.0.0.0 > sglang.log 2>&1 &
sleep 45

# 3. Run pipeline: SGLang â†’ Ray â†’ GRPO (use GPU 0)
CUDA_VISIBLE_DEVICES=0 python scripts/run_ray_pipeline.py
```

### Evaluate
```bash
# Serve trained model
pkill -f sglang && sleep 2
CUDA_VISIBLE_DEVICES=1 nohup python -m sglang.launch_server \
    --model-path ./outputs/grpo_model \
    --port 30000 --host 0.0.0.0 --trust-remote-code > sglang.log 2>&1 &
sleep 45

# Run Pass@k evaluation
python scripts/eval_pass_at_k.py \
    --model http://localhost:30000 \
    --dataset gsm8k \
    --k 1 4 8 \
    --num-problems 100
```

## ğŸ“ Project Structure
```
distributed-reasoning-loop/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generator/
â”‚   â”‚   â”œâ”€â”€ cot_generator.py          # SGLang/vLLM inference
â”‚   â”‚   â”œâ”€â”€ synthetic_data_pipeline.py
â”‚   â”‚   â””â”€â”€ dataset_loader.py         # GSM8K, HumanEval, MATH, MBPP
â”‚   â”œâ”€â”€ verifier/
â”‚   â”‚   â”œâ”€â”€ math_verifier.py          # SymPy symbolic verification
â”‚   â”‚   â””â”€â”€ code_verifier.py          # Docker sandbox execution
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”‚   â”œâ”€â”€ ray_workers.py            # Distributed processing
â”‚   â”‚   â””â”€â”€ kafka_streaming.py        # Streaming pipeline
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ grpo_trainer.py           # Group Relative Policy Optimization
â”‚   â”‚   â”œâ”€â”€ dpo_trainer.py            # Direct Preference Optimization
â”‚   â”‚   â””â”€â”€ sft_trainer.py            # Supervised Fine-Tuning
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ benchmarks.py             # GSM8K, HumanEval evaluators
â”‚       â””â”€â”€ test_time_compute.py      # Best-of-N, MCTS, Self-Consistency
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_ray_pipeline.py           # Full SGLang â†’ Ray â†’ GRPO pipeline
â”‚   â””â”€â”€ eval_pass_at_k.py             # Pass@k evaluation
â””â”€â”€ config/
    â””â”€â”€ default.yaml                  # Pipeline configuration
```

## ğŸ”§ Key Components

### 1. SGLang Generation
- **RadixAttention**: Automatic prefix caching for shared prompts
- **Batched inference**: Concurrent requests via ThreadPoolExecutor
- **Multi-path sampling**: 10 reasoning paths per problem

### 2. Ray Distributed Verification
- **Parallel workers**: 4 actors processing chunks
- **Math verification**: SymPy symbolic comparison
- **Code verification**: Docker sandbox (256MB RAM, 30s timeout)

### 3. GRPO Training (DeepSeek-R1 Approach)
- **No reward model**: Uses group-relative advantages
- **Verification-based**: Correct = positive, incorrect = negative
- **Efficient**: LoRA (r=16, alpha=32), 8-bit quantization

## ğŸ“Š Pipeline Stats
```
Dataset:           GSM8K (500 problems)
Paths generated:   5,000 (10 per problem)
Correct paths:     1,162 (23%)
Incorrect paths:   3,838 (77%)
DPO pairs:         2,085
Training epochs:   5
Final loss:        -0.0249
```

## ğŸ“š References

- [DeepSeek-R1](https://arxiv.org/abs/2401.02954) - GRPO algorithm
- [SGLang](https://github.com/sgl-project/sglang) - RadixAttention inference
- [Ray](https://ray.io/) - Distributed computing
- [GSM8K](https://arxiv.org/abs/2110.14168) - Math reasoning benchmark
